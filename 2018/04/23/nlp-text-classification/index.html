<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/favicon.png"/>
	 <link rel="shortcut icon" href="/img/favicon.png">
	
			
    <title>
    Unique AI Lab
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    
    	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>

			    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  



	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">UNIQUE AI</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/GAN-图像翻译-CV/">GAN 图像翻译 CV</a></li><li><a class="category-link" href="/categories/强化学习/">强化学习</a></li><li><a class="category-link" href="/categories/机器学习/">机器学习</a></li><li><a class="category-link" href="/categories/深度学习/">深度学习</a></li><li><a class="category-link" href="/categories/深度学习/计算机视觉/">计算机视觉</a></li><li><a class="category-link" href="/categories/自然语言处理/">自然语言处理</a></li><li><a class="category-link" href="/categories/自然语言处理/NLP/">NLP</a></li><li><a class="category-link" href="/categories/自然语言处理/NLP/机器学习/">机器学习</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="简介">
		                简介
		            </a>
		        </li>
		        
		        <li>
		            <a href="/project/" title="项目">
		                项目
		            </a>
		        </li>
		        
		        <li>
		            <a href="/member/" title="成员">
		                成员
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
		            
		                <li><a href="https://github.com/UNIQUE-AILAB" class="icon fa-github"><span class="label">GitHub</span></a></li>
		            
		            
		            
		            
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(https://cdn-images-1.medium.com/max/1600/1*HgXA9v1EsqlrRDaC_iORhQ.png);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 1rem ;"><h2>NLP 文本分类问题</h2><h3>Hongxin Liu</h3></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><p>在机器学习任务中，特征工程是最繁琐最耗时的。有人认为特征决定了结果的上限，而模型则是去逼近这个上限。文本分类任务中，特征工程通常包括文本预处理、文本表达、特征提取。<br><a id="more"></a></p>
<h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><p>文本预处理通常包括去标点、停顿词 (stop words)。当然具体应用场景应该具体分析，比如英文分词考虑统一各时态的表达，包含外文词汇时翻译成目标语言，对特殊的词做正则（比如在 Toxic Comment Classification Challenge 比赛中对 fuuuuck 这样的词做特殊匹配），有时甚至不需要去除停顿词（比如 you, my 等词在谩骂时经常出现）。介绍几个 Python 文本预处理的库：<a href="https://github.com/pytorch/text" target="_blank" rel="noopener">torchtext</a>, <a href="https://github.com/nltk/nltk" target="_blank" rel="noopener">nltk</a>。</p>
<h2 id="文本表达"><a href="#文本表达" class="headerlink" title="文本表达"></a>文本表达</h2><p>文本分类问题多采用词的粒度的特征，所以这里主要讲词的表达。常见的表达方式有:</p>
<ul>
<li>one-hot representation: 例如词袋模型(BOW, Bag Of Words)，就采用这种稀疏表达。缺点是向量维度随词的增多可能爆炸，并且每个词都是独立的，难以表达语义信息。</li>
<li>distributed representation: 例如词嵌入(Word Embeddings)，采用这种稠密表达。它基于上下文，包含更丰富的语义信息。常见的模型有<a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="noopener">GloVe</a>, <a href="https://arxiv.org/pdf/1310.4546" target="_blank" rel="noopener">word2vec</a>。在实践中可能还会使用 char embedding, n-gram embedding。</li>
</ul>
<h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>传统的特征提取通常包括特征选择和权重计算。权重计算通常用TF-IDF及其变体，主要思路是：字词的重要性随着它在文本中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。<br>而深度神经网络解决了自动特征提取。</p>
<h2 id="其他-trick"><a href="#其他-trick" class="headerlink" title="其他 trick"></a>其他 trick</h2><p>实际的特征工程中还有许多有用的方法，比如数据增强 (Data Augmentation)。这里介绍一些文本分类中数据增强的方法：</p>
<ul>
<li>加噪声。如删减一些词，打乱词序，分隔合成词 (split combined words)。</li>
<li>翻译成其他语言再翻译回来。Kaggle 上关于这个方法的讨论：<a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038" target="_blank" rel="noopener">A simple technique for extending dataset</a></li>
<li>伪标签 (pseudo-labelling)。在训练集和测试集数据分布不一致时，这种方法尤为有效。它的运行过程如下图：<br><img src="https://ver217-1253339008.cos.ap-shanghai.myqcloud.com/blog-img/pseudo-labeling.png" alt="pseudo-labelling"><br>一篇介绍它的论文：<a href="http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf" target="_blank" rel="noopener">Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks</a></li>
<li>拼接词汇：把一些词拼接成一个短文本进行训练，如在 Mercari Price Suggestion Challenge 中把名称和品牌 concat 成一个短文本。</li>
<li>label embedding：在多任务学习中使用 label embedding，提取标签的语义信息，利于迁移学习。详情见论文<a href="https://arxiv.org/abs/1710.07210" target="_blank" rel="noopener">Multi-Task Label Embedding for Text Classification</a></li>
</ul>
<h1 id="文本分类模型"><a href="#文本分类模型" class="headerlink" title="文本分类模型"></a>文本分类模型</h1><h2 id="常见模型"><a href="#常见模型" class="headerlink" title="常见模型"></a>常见模型</h2><ol>
<li>fastText</li>
<li>TextCNN</li>
<li>TextRNN</li>
<li>TextRCNN</li>
<li>Hierarchical Attention Network</li>
<li>seq2seq with attention</li>
<li>Transformer (Attention Is All You Need)</li>
<li>Dynamic Memory Network</li>
</ol>
<h2 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h2><h3 id="fastText"><a href="#fastText" class="headerlink" title="fastText"></a>fastText</h3><p>fastText 并不是用深度学习解决文本分类的主流方法，但是它非常简单，而且特别快。<a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="noopener">Bag of Tricks for Efficient Text Classification</a>。模型如下图：<br><img src="https://ver217-1253339008.cos.ap-shanghai.myqcloud.com/blog-img/fastText.png" alt="fastText"><br>原理是把句子的词向量做了平均然后接一个线性分类器，文中使用 softmax 来计算概率。为了加速训练，文中还用了层次化 softmax (Hierarchical softmax)，这是一种基于Huffman树的结构，详情见论文。文中还加入了一些 n-gram 特征来捕获关于词序的局部特征。值得一提的是，它的训练速度极快，即使用 CPU 训练，通常也比 CNN 方法快很多倍。Facebook 开源了这个库 <a href="https://github.com/facebookresearch/fastText" target="_blank" rel="noopener">fastText</a></p>
<h3 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h3><p>原文链接：<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a>，还有一篇关于它的 Guide: <a href="https://arxiv.org/abs/1510.03820" target="_blank" rel="noopener">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</a><br>原理见下图：<img src="https://arxiv-sanity-sanity-production.s3.amazonaws.com/render-output/91804/x1.png" alt="TextCNN"><br>原文的配置如下：</p>
<table>
<thead>
<tr>
<th>Description</th>
<th>Values</th>
</tr>
</thead>
<tbody>
<tr>
<td>input word vectors</td>
<td>Google word2vec</td>
</tr>
<tr>
<td>feature maps</td>
<td>100</td>
</tr>
<tr>
<td>activation function</td>
<td>ReLU</td>
</tr>
<tr>
<td>pooling</td>
<td>1-max pooling</td>
</tr>
<tr>
<td>dropout rate</td>
<td>0.5</td>
</tr>
<tr>
<td><em>l2</em> norm constraint</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>文中的词向量有 static 和 non-static 两种，即一种直接使用预训练的词向量，一种在训练网络的过程中同时调整词向量。使用单 filter 准确率高的 filter 组合能获得更好的效果。Github 上有许多实现可以参考：<a href="https://github.com/Shawn1993/cnn-text-classification-pytorch" target="_blank" rel="noopener">cnn-text-classification-pytorch</a>, <a href="https://github.com/DongjunLee/text-cnn-tensorflow" target="_blank" rel="noopener">text-cnn-tensorflow</a><br>还有一些文章使用 k-max pooling，如<a href="https://arxiv.org/pdf/1404.2188.pdf" target="_blank" rel="noopener">A Convolutional Neural Network for Modelling Sentences</a>，即保留 k 个最大值。它可以保留局部信息。在实践中通常加入 attention，可参考 ABCNN。</p>
<h3 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h3><p>TextRNN 的原理可以参考：<a href="https://arxiv.org/abs/1605.05101" target="_blank" rel="noopener">Recurrent Neural Network for Text Classification with Multi-Task Learning</a><br><img src="https://arxiv-sanity-sanity-production.s3.amazonaws.com/render-output/96829/x1.png" alt="Recurrent Neural Network for Classification"><br>文中把 word embedding 输入给双向 LSTM 后直把 $h_T$ 输给 FC 然后 softmax 输出，使用 Adagrad 训练，Loss 为交叉熵。</p>
<h3 id="TextRCNN"><a href="#TextRCNN" class="headerlink" title="TextRCNN"></a>TextRCNN</h3><p>原文链接：<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="noopener">Recurrent Convolutional Neural Networks for Text Classification</a><br>这篇文章用单词本身和上下文了来表示一个词。它的原理如下图：<img src="https://ver217-1253339008.cos.ap-shanghai.myqcloud.com/blog-img/TextRCNN.png" alt="TextRCNN"><br>它使用双向 RNN 来获取一个词的上下文，其中：<br>$$c_l(w_i) = f(W^{(l)}c<em>l(w</em>{l-1}) + W^{(sl)}e(w_{i-1})))$$ $$c_r(w_i) = f(W^{(r)}c<em>r(w</em>{r+1}) + W^{(sr)}e(w_{i+1})))$$<br>然后把它们 concat 起来得到词的表示：$$x_i = [c_l(w_i);e(w_i);c_r(w_i)]$$<br>再通过$y_i^{(2)} = tanh(W^{(2)}x_i + b^{(2)})$ 得到 $y_i^{(2)}$，它是潜在的语义向量。再接 max-pooling，最后 FC + softmax 输出。从卷积神经网络的角度看，前面提到的循环结构相当于卷积层。在训练时最大化它的极大似然函数，并且用 SGD 优化。</p>
<h3 id="Hierarchical-Attention-Network"><a href="#Hierarchical-Attention-Network" class="headerlink" title="Hierarchical Attention Network"></a>Hierarchical Attention Network</h3><p>原文链接：<a href="http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">Hierarchical Attention Networks for Document Classification</a><br>本文在 attention 机制的基础上提出了层次化的结构，即由词向量得到句子向量，由句子向量得到文档向量，并在层间加入 attention。<br><img src="https://ver217-1253339008.cos.ap-shanghai.myqcloud.com/blog-img/Hierarchical-Attention-Network.png" alt="Hierarchical Attention Network"><br>本文使用基于 GRU 的序列编码器，它有重置门 (reset gate) 和更新门 (update gate)，两个门一起决定有多少信息要更新。<br>首先将词转化为词向量，再通过双向 GRU ，把结果 concat 在一起得到词的表示：<br>$$x_{it} = W<em>ew</em>{it}$$ $$\overrightarrow{h<em>{it}} = \overrightarrow{GRU}(x</em>{it})$$ $$\overleftarrow{h<em>{it}} = \overleftarrow{GRU}(x</em>{it})$$ $$h<em>{it} = [\overrightarrow{h</em>{it}}, \overleftarrow{h<em>{it}}]$$<br>attention 机制是要找到句子中对含义贡献最大的词，$u</em>{it}$ 为 $h<em>{it}$ 的隐含表示：<br>$$u</em>{it} = tanh(W<em>wh</em>{it} + b<em>w)$$ $$\alpha</em>{it} = \frac{exp(u_{it}^Tu_w)}{\sum<em>texp(u</em>{it}^Tu_w)}$$ $$s_i = \sum<em>t\alpha</em>{it}h_{it}$$<br>$u<em>w$ 为随机初始化的上下文向量，$\alpha</em>{it}$ 为 attenttion 的权重矩阵，表示第 i 个句子的第 j 个词。$s_i$ 为下一层的输入。句子级别和文档级别的表示与词类似。</p>
<h1 id="实践经验"><a href="#实践经验" class="headerlink" title="实践经验"></a>实践经验</h1><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p>学术和工程实践的 Gap 往往很大，学术考虑模型的设计，实践中还要考虑效率、性价比等等。实践中往往先用 CNN 做到比较好的结果再来改进模型，由于 RNN 提升的不是特别明显，但是训练开销巨大，在限时赛中经常使用 CNN 及其改进模型。</p>
<h2 id="防止过拟合"><a href="#防止过拟合" class="headerlink" title="防止过拟合"></a>防止过拟合</h2><p>除了常用的方法，值得一提的是 dropout 往往很有效，默认的参数0.5在很多情况下效果不错。如果数据集很小或者用了更好的方法，比如 batch norm，可以不用 dropout。</p>
<h2 id="关注每一次迭代"><a href="#关注每一次迭代" class="headerlink" title="关注每一次迭代"></a>关注每一次迭代</h2><p>善用 tensorboard, visdom 等可视化工具，关注每一次迭代的质量。必要的时候可以手动调整每个 epoch 的 batch_size, optimizer, lr 等参数。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>词向量的 fine-tuning 很重要。迭代时默认开启 shuffle。</p>
<h2 id="超参数调节"><a href="#超参数调节" class="headerlink" title="超参数调节"></a>超参数调节</h2><p>上文提到的 <a href="https://arxiv.org/abs/1510.03820" target="_blank" rel="noopener">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</a> 详细的介绍了超参数调节的实验过程，可以参考这篇文章。</p>
<h2 id="组合激活函数"><a href="#组合激活函数" class="headerlink" title="组合激活函数"></a>组合激活函数</h2><p>每一种激活函数都会丧失一部分信息，可以把不同的激活函数 concat 起来，通过梯度自动选择适合的激活函数。</p>
<h2 id="类别不均衡"><a href="#类别不均衡" class="headerlink" title="类别不均衡"></a>类别不均衡</h2><ul>
<li>欠采样：丢弃一些反例，使得正、反例数目接近。</li>
<li>过采样：增加一些正例，使得正、反例数目接近，即数据增强。</li>
<li>阈值移动：$m^+$, $m^-$ 分别为正、反例数目，$y$ 为正例的可能性，通常认为若 $\frac{y}{1-y} &gt; 1$，即 $y &gt; 0.5$ 时预测为正例，其中 0.5 即为阈值。类别不均衡时，$\frac{y}{1-y} &gt; \frac{m^+}{m^-}$ 时才预测为正例，可以看出正、反例 1:1 时阈值为 0.5 才是合理的。</li>
</ul>
<h2 id="实现参考"><a href="#实现参考" class="headerlink" title="实现参考"></a>实现参考</h2><p><a href="https://github.com/brightmart/text_classification" target="_blank" rel="noopener">brightmart/text_classification</a> 这里有文本分类深度学习方法全套合集。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf" target="_blank" rel="noopener">Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1710.07210" target="_blank" rel="noopener">Multi-Task Label Embedding for Text Classification</a></li>
<li><a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="noopener">Bag of Tricks for Efficient Text Classification</a></li>
<li><a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></li>
<li><a href="https://arxiv.org/abs/1510.03820" target="_blank" rel="noopener">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</a></li>
<li><a href="https://arxiv.org/pdf/1404.2188.pdf" target="_blank" rel="noopener">A Convolutional Neural Network for Modelling Sentences</a></li>
<li><a href="https://arxiv.org/abs/1605.05101" target="_blank" rel="noopener">Recurrent Neural Network for Text Classification with Multi-Task Learning</a></li>
<li><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552" target="_blank" rel="noopener">Recurrent Convolutional Neural Networks for Text Classification</a></li>
<li><a href="http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">Hierarchical Attention Networks for Document Classification</a></li>
<li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a></li>
<li><a href="https://arxiv.org/abs/1506.07285" target="_blank" rel="noopener">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a></li>
</ol>

            </div>

            <!-- Post Comments -->
            

        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <span>Unique AI Lab</span><br/>
            
            	<span id="busuanzi_container_site_pv">2018总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>

    </div>
</body>




 	
</html>
